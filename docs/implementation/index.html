<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-gb">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Implementation - Bootstrapping a Secure Infrastructure</title>
    <meta name="generator" content="Hugo 0.36.1" />

    
    <meta name="description" content="Bootstrapping a Secure Infrastructure">
    
    <link rel="canonical" href="/grim/implementation/">
    
    <meta name="author" content="Oliver Mussell">
    

    <meta property="og:url" content="/grim/implementation/">
    <meta property="og:title" content="Bootstrapping a Secure Infrastructure">
    <meta property="og:image" content="/grim/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Bootstrapping a Secure Infrastructure">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="/grim/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="/grim/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('/grim/fonts/icon.eot');
        src: url('/grim/fonts/icon.eot')
               format('embedded-opentype'),
             url('/grim/fonts/icon.woff')
               format('woff'),
             url('/grim/fonts/icon.ttf')
               format('truetype'),
             url('/grim/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="/grim/stylesheets/application.css">
    <link rel="stylesheet" href="/grim/stylesheets/temporary.css">
    <link rel="stylesheet" href="/grim/stylesheets/palettes.css">
    <link rel="stylesheet" href="/grim/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,700|Roboto&#43;Mono">
    <style>
      body, input {
        font-family: 'Roboto', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Roboto Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="/grim/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-grey palette-accent-dark purple">




<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Implementation
      </div>
    </div>

        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>

</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="/grim/" class="project">
    <div class="banner">
      
        <div class="logo">
          <img src="/grim/images/logo.png">
        </div>
      
      <div class="name">
        <strong>Bootstrapping a Secure Infrastructure </strong>
        
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Overview" href="/grim/overview/">
	
	Overview
</a>



  
</li>



<li>
  
    



<a  title="Design" href="/grim/design/">
	
	Design
</a>



  
</li>



<li>
  
    



<a class="current" title="Implementation" href="/grim/implementation/">
	
	Implementation
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Homelab" href="/grim/homelab/">
	
	Homelab
</a>



  
</li>


        </ul>
        
      </div>
    </div>
  </div>
</nav>

	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Implementation </h1>

			

<h1 id="bootstrapping-a-secure-infrastructure">Bootstrapping a Secure Infrastructure</h1>

<p><em>Oliver Mussell - 2016-2018</em></p>

<h1 id="implementation">Implementation</h1>

<p>The architecture described in the design is only aimed at the infrastructure setup, not application servers. Each of the services provided can be accessed by other architectures based on different operating systems. So for example, Windows and Linux infrastructures would still be able to query the DNS service without any extra configuration.</p>

<h2 id="stricthostkeychecking-vs-verifyhostkeydns-problem">StrictHostKeyChecking vs VerifyHostKeyDNS Problem:</h2>

<h3 id="problem-statement">Problem Statement</h3>

<p>Do the StrictHostKeyChecking and VerifyHostKeyDNS options in ssh_config work together?</p>

<ul>
<li>StrictHostKeyChecking - If set to yes, ssh will never automatically add host keys to the known_hosts file and refuses to connect to hosts whose host key has changed(This is the preferred option). The host keys of known hosts will be verified automatically in all cases.</li>
<li>VerifyHostKeyDNS - Specifies whether to verify the remote key using DNS and SSHFP resource records. If set to yes, the client implicitly trusts keys that match a secure fingerprint from DNS. Insecure fingerprints will be handled as if this option was set to ask. If this option is set to ask, information on fingerprint match will be displayed, but the user will still need to confirm new host keys according to the StrictHostKeyChecking option.</li>
</ul>

<p>So if the fingerprint is presented from insecure DNS (not DNSSEC validated), or if the SSHFP record does not exist, does it prompt the user? We don&rsquo;t want this to happen since these SSH connections are happening autonomously.</p>

<p>Also need to check what happens if both options are set to yes.</p>

<p>So the host key is verified via DNS. If the fingerprint is correct it will connect. If it is incorrect, it will follow the StrictHostKeyChecking option, which when set to yes will refuse to connect to a host if its host key has changed.</p>

<p>If a host key is verified through DNS, is it still added to known_hosts?
No, the host keys are only stored in the SSHFP records. This also means that when the host key is rotated, the SSHFP record needs to be updated once rather than having to amend the known_hosts file on every server that has ever connected.</p>

<h2 id="ssh-host-key-rotation">SSH Host Key Rotation</h2>

<p>How do you rotate SSH host keys and update the SSHFP records in DNS(SEC)?</p>

<p>Requirement for renewal is initiated every x days/weeks
host generates new host keys
host connects to control machine, provides the new SSHFP values and asks for hosts SSHFP records to be updated
control machine updates DNS zone info with new SSHFP values
if update is successful, remove old values and resign zone. Inform host to remove old host keys
if update is unsuccessful, remove new values and report error.</p>

<h2 id="setting-up-ipsec-problem">Setting up IPsec Problem:</h2>

<h3 id="problem-statement-1">Problem Statement</h3>

<p>How do we set up IPsec between the control machine and machines it creates?</p>

<p>How do we set up IPsec between the router and machines communicating with it?</p>

<p>How do we set up IPsec between the router and other sites?</p>

<h2 id="assigning-ip-addresses-to-jails">Assigning IP addresses to jails</h2>

<p>It is not possible to assign IP addresses to jails using DHCP, they can only be assigned via jail.conf. This introduces the issue that a human would be required to find out a free address and manually enter it into the jail.conf, which becomes additionally complex with long and hard to remember IPv6 addresses. Some of this problem can be mitigated using variables for example by having the subnet prefix as a $PREFIX variable which can then be referenced as e.g. $PREFIX::d3d8.</p>

<p>Another method available is the ip_hostname parameter in jail.conf which resolves the host.hostname parameters in DNS and all addresses returned by the resolver are added for the jail. So instead of entering the IP into jail.conf, a AAAA record would be manually entered into DNS and the jail would pick it up from there.</p>

<p>Since only applications that require an external IP address are hosted inside jails, those applications should have a known IP address. This would be services like DNS, which needs a static and human-known IP address. These IP addresses could be hard coded into the DNS record available at the temporary DNS server (unbound) hosted on the control machine which is available during initial bootstrapping. The jails would then use the ip_hostname parameter to lookup their hostname in DNS, from which they would assign the jails IP address. Subsequent hosts would generate their IP address via SLAAC and DNS would be updated via puppet as normal.</p>

<h2 id="giving-dns-server-information-to-clients">Giving DNS server information to clients</h2>

<p>Rather than manually configuring /etc/resolv.conf for the location of the local DNS servers, this information can be provided either by the router or DHCP server. If you do not want to run a DHCP server and rely solely on SLAAC for address allocation, then you can have the router provide the DNS information. Otherwise, the DHCP server can provide the DNS information. <a href="https://tools.ietf.org/html/rfc8106">RFC8106</a></p>

<p>The major benefit of this approach is that you do not have to make any manual configurations for the location of the DNS servers on any of your clients. However, one of the drawbacks is experienced during the initial bootstrap when the DNS servers do not yet exist. So the DNS servers will need to have their stub resolvers configured manually.</p>

<p>Since we have a robust DNSSEC implementation available, it makes sense to store as much crypto/public keys in there as possible rather than having them spread over multiple mechanisms. So rather than TLS certs in 3rd party PKI, SMIME certs in LDAP, SSH host keys in known_hosts files and IPsec public keys distributed manually, you can have TLSA, SMIMEA, SSHFP and IPSECKEY Resource Records stored securely in DNS.</p>

<p>It is also important for us to have this information in DNS so that it can potentially be referenced by other organisations. If another org needs to access a website, it needs to be secured with TLS which is validated by the TLSA record via DANE. Likewise, if a person in another organisation wants to send an email, they need to know the TLS cert to secure the TLS communication and also the SMIME certificate to encrypt the email itself. By publishing the TLSA record and SMIMEA records in DNS, the other organisation can access this information and be confident that the records are accurate.</p>

<h2 id="ip-addresses-that-need-to-be-known-by-a-human">IP addresses that need to be known by a human</h2>

<ul>
<li>Router(s)</li>
<li>Control Machine(s)</li>
<li>DNS servers</li>
</ul>

<h2 id="resource-records-to-be-stored-in-dns-sec">Resource records to be stored in DNS(SEC):</h2>

<p>Static:</p>

<ul>
<li>DNS servers</li>
<li>Hostname to static IP for infrastructure servers</li>
<li>CNAMES for standard services (auth.example.com, dns.example.com)</li>
<li>MX records</li>
</ul>

<p>Dynamic:</p>

<ul>
<li>Hostname to dynamic IP for app/other servers</li>
<li>SSHFP records</li>
<li>IPSECKEY records</li>
<li>TLSA records (for all web/app servers)</li>
<li>SMIMEA (for each user)</li>
</ul>

<h2 id="ensure-packages-and-services-continue-working-after-os-package-updates">Ensure packages and services continue working after OS / package updates</h2>

<p>It would be preferable to have updates to the OS and packages to be downloaded, tested, and applied automatically. Usually this is one of the tasks of systems administrators, and is often carried out at regular intervals and applying updates all at once. This also goes against the immutable infrastructure paradigm that is often employed in cloud infrastructures.</p>

<p>Using continuous integration tools and configuration management tools in parrallel will allow us to perform these actions autonomously.</p>

<p>Very often, you can subscribe to mailing lists which announce when OS or package updates are released. CI tools like Builtbot can listen for these emails, and run specific actions based on the content. For example, when new security updates for the kernel or base are released, an email would be sent and received by Buildbot which would then kick off the patching of  servers in the test environment. Once tested and the applications are confirmed to be working ok, then the patches can be applied to other environments.</p>

<p>Likewise if a new major version is released, an email is received, Buildbot then runs through the process of creating a new zfs boot environment on servers in the test environment and applies the new major version to the new BE. After rebooting into the new environment, tests are performed to check that the application still works ok. If so, the major version is rolled out to other environments. This ties in with the need for redundancy, because we would need to take a server out from the load balancer to perform the upgrade and put it back once tested and working.</p>

<p>By creating infrastructure acceptance tests using tools like testinfra, we can easily validate that packages, services and configuration files continue to work in the same way before and after upgrades take place. It also gives us the opportunity to practice test-driven infrastructure, by first creating the test then the code to actually implement the change. These tests will also contribute to cross OS compatibility because the same tests can be run on different OSes.</p>

<h2 id="cross-os-init-service-compatibility">Cross OS init/service compatibility</h2>

<p>Init systems vary wildly across different operating systems. FreeBSD has a sane init system based on shell scripts that is incredibly easy to port to other OSes, but this is not true for systemd and the like. One option is to use daemontools by djb which has packages on most OSes and is designed specifically to be cross-OS compatible.</p>

<p>It can also use shell scripts, so it may be possible to just copy the FreeBSD service scripts and use them with daemontools but this needs testing.</p>

<h1 id="bootstrap">Bootstrap</h1>

<h2 id="version-control">Version Control</h2>

<p>There are a number of options including git and subversion. Choose the tool that is best suited to your organisation. git has been chosen as it is open source, familiar to most people and easy to pick up.</p>

<p>Since Git is a collaborative tool, it is common to install a web version of git such as GitLab or Gogs to give people a GUI. This is organisation specific, for our use case we will just have git repos stored on a specific server/storage area. All of the tools available to git are usable in the git package.</p>

<p>Modern configuration management systems have the ability to use git repositories as backends for their configuration files. This allows a workflow of only ever updating files that exist in version control which means changes are entered into history and can be audited.</p>

<p>git package installed
/usr/local/git ? contains repos
depends on ssh infrastructure
separate user accounts are used for specific projects
machines access these accounts with their specifically generated ssh key pair, with the public key put into the authorized keys of the user for the project</p>

<p>salt-repo authorized_keys:
  salt-master01 public key
  salt-master02 public key</p>

<p>hugo-repo authorized_keys:
  hugo-master01 public key
  hugo-master02 public key</p>

<p>These service account users have git-shell as their shell, which means they can only push/pull.</p>

<h2 id="ssh">SSH</h2>

<p>The default for HostKey is to accept RSA, DSA, ECDSA and ED25519 host keys. We only want to use ED25519, which can be enabled with:
HostKey /etc/ssh/ssh_host_ed25519_key</p>

<p>We need to have the capability to rotate the host key. The old and new keys both need to be accepted until the new keys SSHFP resource record has been published into DNS.</p>

<p>So to rotate the host key:</p>

<ul>
<li>Generate a new host key</li>
<li>Update the /etc/ssh/sshd_config file to specify the name of the new host key, while keeping the existing host key defined as well</li>
<li>Calculate the SSHFP resource record of the new host key</li>
<li>Update the zone file with the SSHFP record</li>
<li>Resign the zone</li>
<li>Publish the zone</li>
<li>Update the /etc/ssh/sshd_config file to remove the old host key</li>
</ul>

<p>The node itself should be responsible for generating the new host keys so that the private keys are not transported across the network. Using Saltstack&rsquo;s <a href="https://docs.saltstack.com/en/getstarted/event">Event-Driven Infrastructure</a> model we can detect when a new host key is generated and automatically perform the steps required to update the zone with the new SSHFP record.</p>

<p>The default for AuthorizedKeysFile is to use the .ssh/authorized_keys file in the users home directory.
AuthorizedKeysFile .ssh/authorized_keys</p>

<p>Like host keys, user keys should be rotated regularly. For service accounts, the rotation can follow a similar process to host key rotation.</p>

<ul>
<li>Generate a new user key</li>
<li>Update ssh_config to use the new key, while keeping the existing user key defined as well</li>
<li>Grab the public key, and update the authorized_keys files on machines as necessary</li>
<li>Once updated, update the ssh_config file to remove the old user key</li>
</ul>

<p>We will keep this default, but the way that users connect may be slightly different than what is normally expected. For example, on a git server we may have a particular user account created to allow access to a repository. It is likely that this same repository would be accessed by many different machines to pull down their configuration. Rather than creating a separate user account per machine on the git server, we would create one account called git-repo or something, then the authorized_keys file for that user would contain the public keys of multiple machine users. So the master01 machine would have a git-repo user as well, but its SSH keys would be different to the git-repo users SSH keys on the master02 machine. But the public keys of the git-repo user of both master01 and master02 would exist in the authorized_keys file on the git server.</p>

<p>!! none of that makes sense and needs rewriting !!</p>

<p>It is a standard practice to use service accounts where an account is required to carry out a particular process with restricted permissions and without human intervention. One of the common issues however, is that often the amount of such accounts increases dramatically as every application and workflow requires a new service account. This quickly becomes unmanageable and you end up with lots of unused accounts that have access to many different things and no way of knowing what they are doing.</p>

<p>To combat this, we want to standardise the accounts as much as possible while also limiting access permissions and auditing actions that are taken.</p>

<p>For example, a common action is to pull and push code to remote git repositories. In order to do this, a user account with the correct SSH key pair needs to connect to the server hosting the repository and have access to the directories containing the repositories. Rather than having a specific account for each application or host and splitting up the permissions for push and pull, we can instead have one account &lsquo;git_remote&rsquo; which is an account which exists on every server. Its home directory is /usr/local/git, and only this user can access it. The shell for the &lsquo;git_remote&rsquo; user is set to &lsquo;git-shell&rsquo; which  allows git push and pull operations only.</p>

<p>Though the &lsquo;git_remote&rsquo; account exists on every server the SSH key pair is unique per server. In order to allow a server to access a repo on another server, the public key of the &lsquo;git_remote&rsquo; user can be added to the authorized_keys file of the &lsquo;git_remote&rsquo; user on the machine hosting the repositories. So access control is maintained using normal SSH keys and there is a standard maintainable way of performing this common task that works in the same way across the whole infrastructure.</p>

<p>In addition, the SSH keys of the accounts can be rotated regularly as per normal.</p>

<p>Each of the components that are required to maintain this approach are simple and common:</p>

<ul>
<li>Managing standard Unix user accounts</li>
<li>Managing directory permissions</li>
<li>Managing SSH keys</li>
<li>Managing authorized_keys files</li>
</ul>

<p>machines access these accounts with their specifically generated ssh key pair, with the public key put into the authorized keys of the user for the project</p>

<p>salt-repo authorized_keys:
  salt-master01 public key
  salt-master02 public key</p>

<p>hugo-repo authorized_keys:
  hugo-master01 public key
  hugo-master02 public key</p>

<p>The SSH keys used by humans to directly connect to servers are harder to manage, since we would probably want the private keys to be encrypted and require a password or preferably include two-factor authentication as well. So its harder to automate the rotation. It may be best to instead have a MOTD that encourages the user to update their SSH key. It could give a time period that the user needs to have changed their keys by, and dynamically update the MOTD to say if the keys are &ldquo;expired&rdquo;. If we were really strict, we could block log in at this point. However, the infrastructure is supposed to be hands-off and there should only be a requirement for a human to log into a server directly if things have gone wrong. So we dont really want to block human access.</p>

<p>Required SSH accounts / connections</p>

<ul>
<li>git pull/push - config management and application repos</li>
<li>zfs send/recv - upgrading zfs boot environments. application deploys via zfs/jails</li>
<li>human user accounts - human access to servers</li>
</ul>

<p>Config files were based on <a href="https://wiki.mozilla.org/Security/Guidelines/OpenSSH">https://wiki.mozilla.org/Security/Guidelines/OpenSSH</a></p>

<p>In FreeBSD, the sshd_config and ssh_config files are used to determine the behaviour of the ssh daemon and client respectively. By default, the options are declared in the file but are commented out so that it is easy to see what the default value of a particular option is. Uncommented options are then used to override behaviour.</p>

<p>For clarity, we can remove all of the commented lines in this file. Any lines that remain would then be only those options that override behaviours.</p>

<pre><code>sshd_config

HostKey /etc/ssh/ssh_host_ed25519_key
KexAlgorithms curve25519-sha256@libssh.org
Ciphers chacha20-poly1305@openssh.com
AuthenticationMethods publickey
LogLevel VERBOSE
PermitRootLogin No
UsePrivilegeSeparation sandbox
</code></pre>

<p>There are some options that are currently not shown in the below ssh_config. These would include the IdentityFile option, to determine the key pair to be used with a specific account.</p>

<p>The below example would reside at /etc/ssh/ssh_config to set the standard default options, and the IdentityFile and any other user specific options would be specified in the users ~/.ssh/ssh_config file.</p>

<pre><code>ssh_config

HashKnownHosts yes
HostKeyAlgorithms ssh-ed25519-cert-v01@openssh.com,ssh-ed25519
KexAlgorithms curve25519-sha256@libssh.org
Ciphers chacha20-poly1305@openssh.com
</code></pre>

<pre><code>su -m git_remote -c 'ssh-keygen -t ed25519 -N &quot;&quot; -f /usr/local/git/.ssh/id_ed25519 -C git_remote@testjail1'
</code></pre>

<p>also need to disable interactive login as per <code>man git-shell</code></p>

<h1 id="implementation-1">Implementation</h1>

<p>Throughout this design it assumed that the infrastructure is built on bare metal by default, though it is acceptable that multiple services may be running on the same physical host and segregated using jails or virtual machines. Cloud infrastructures are not considered because they are inherently hosted externally which is not possible in a secure environment.</p>

<h1 id="provisioning">Provisioning</h1>

<h2 id="ipv6">IPv6</h2>

<h3 id="address-autoconfiguration-slaac">Address Autoconfiguration (SLAAC)</h3>

<h3 id="send">SEND</h3>

<h3 id="send-savi">SEND SAVI</h3>

<h3 id="dhcpv6">DHCPv6</h3>

<h3 id="ipsec">IPsec</h3>

<h2 id="os">OS</h2>

<h3 id="freebsd">FreeBSD</h3>

<h3 id="jails">Jails</h3>

<h3 id="zfs">ZFS</h3>

<p><a href="https://www.freebsd.org/cgi/man.cgi?query=zfsd">https://www.freebsd.org/cgi/man.cgi?query=zfsd</a></p>

<p>zfsd will detect if a disk becomes degraded or unavailable and will automatically activate a hot spare if available.</p>

<h3 id="host-install-tools">Host Install Tools</h3>

<p>Download the updates</p>

<p>More testing with pkgbase needs to be carried out, but ideally if it works, we could build a new zfs boot environment, install the requisite packages and boot into it.</p>

<p>While NanoBSD was considered in the past, zfs boot environments are now used.</p>

<p>With zfs boot environments, we can download the updated base files, create a new boot environment, start it in a jail, apply our configuration, test it, and then replicate the zfs dataset to other hosts using zfs send/recv over SSH. This gives us a reliable method of upgrading hosts and also minimises the amount of traffic over the internet. We would only need to download the base files once and then distrubute them locally rather than each host downloading updated versions.</p>

<p>Likewise, any packages that are required for our infrastructure to work should be download onto a local package mirror/cache. This means we can still function if the internet connection is unavailable and allows us to ensure we are using the latest versions of packages.</p>

<p>Until pkgbase becomes a reality, we can just download the images or kernel.txz/base.txz files from the freebsd ftp sites. One alternative is to download the source code via subversion, then compile it yourself. This takes about one and a half hours though. Security patches can be applied by downloading the patch, running the diff through <code>patch</code> and then recompiling. Again, compilation takes ages, so it would be better to just use the images or precompiled binaries.</p>

<h3 id="ad-hoc-change-tools">Ad-Hoc Change Tools</h3>

<h1 id="configuration-management">Configuration Management</h1>

<h2 id="dns">DNS</h2>

<h3 id="dnssec">DNSSEC</h3>

<h3 id="dane">DANE</h3>

<h3 id="dane-smimea-for-email-security">DANE/SMIMEA for Email Security</h3>

<h3 id="s-mime-or-pgp">S/MIME or PGP</h3>

<h2 id="kerberos">Kerberos</h2>

<h2 id="ntp">NTP</h2>

<h3 id="ntpsec">NTPsec</h3>

<h1 id="app-deployment">App Deployment</h1>

<h2 id="application-servers">Application Servers</h2>

<h3 id="nginx">NGINX</h3>

<h1 id="security-and-compliance">Security and Compliance</h1>

<h2 id="security-and-crypto">Security and Crypto</h2>

<h3 id="tls">TLS</h3>

<h3 id="ssh-1">SSH</h3>

<h3 id="hsm">HSM</h3>

<h3 id="passwords">Passwords</h3>

<p>Use one time passwords generator with <code>pass</code></p>

<p>freebsd package is password-store</p>

<h3 id="tcp-wrapper">TCP Wrapper</h3>

<p>TCP wrapper provides host access controls to inetd daemons. It should be used in conjunction with a firewall</p>

<h3 id="ids">IDS</h3>

<p>mtree</p>

<h3 id="firewalls">Firewalls</h3>

<h2 id="configuration-management-tools">Configuration Management Tools</h2>

<h2 id="authorisation-access-control-lists">Authorisation / Access Control Lists</h2>

<h2 id="role-based-access-control-shared-administration-sudo">Role-Based Access Control / Shared Administration (sudo)</h2>

<h2 id="domain-naming-service">Domain Naming Service</h2>

<h2 id="directory-service-ldap">Directory Service (LDAP)</h2>

<h2 id="time-service">Time Service</h2>

<h2 id="logging-auditing">Logging / Auditing</h2>

<p>FreeBSD includes syslog in base. Newsyslog is the equivalent of logrotate, and is used to rotate logs.</p>

<p>/etc/newsyslog.conf</p>

<pre><code># logfilename          [owner:group]    mode count size when  flags [/pid_file] [sig_num]
/path/logs/*.log    644  7     100  $D0   G
/path/logs/*/*.log  644  7     100  $D0   G
</code></pre>

<h2 id="rpc-admin-service">RPC / Admin service</h2>

<h1 id="orchestration">Orchestration</h1>

<h2 id="specific-operational-requirements">Specific Operational Requirements</h2>

<h3 id="configuration">Configuration ###</h3>

<h3 id="startup-and-shutdown">Startup and shutdown ###</h3>

<h3 id="queue-draining">Queue draining</h3>

<h3 id="software-upgrades">Software upgrades</h3>

<h3 id="backups-and-restores">Backups and restores</h3>

<h3 id="redundancy">Redundancy</h3>

<h3 id="replicated-databases">Replicated databases</h3>

<h3 id="hot-swaps">Hot swaps</h3>

<h3 id="access-controls-and-rate-limits">Access controls and rate limits</h3>

<h3 id="monitoring">Monitoring</h3>

<h3 id="auditing">Auditing</h3>

<h2 id="unspecific-operational-requirements">Unspecific Operational Requirements</h2>

<h3 id="assigning-ipv6-addresses-to-clients">Assigning IPv6 Addresses to Clients ###</h3>

<h3 id="static-or-dynamic-ipv6-addresses-dhcpv6-or-slaac">Static or Dynamic IPv6 Addresses (DHCPv6 or SLAAC)</h3>

<h3 id="ipv6-security">IPv6 Security</h3>

<h3 id="hostname-conventions">Hostname Conventions</h3>

<h3 id="choosing-an-operating-system">Choosing an Operating System</h3>

<h3 id="choosing-a-configuration-management-tool">Choosing a Configuration Management Tool</h3>

<h3 id="scheduling-with-cron">Scheduling with cron</h3>

<h1 id="scaling">Scaling</h1>

<h1 id="user-access">User Access</h1>

<h1 id="infrastructure-testing">Infrastructure Testing</h1>

<p>To check that the infrastructure is working as desired a infra testing tool should be used in combination with the configuration management tool. You can use these to validate that the config management config has been applied correctly, to provide test cases to validate any updates to the infra code still work,</p>

<p>testinfra is being used because its based on python. Other tools are serverspec/inspec. They are based on ruby. I&rsquo;m trying to stick to python because it is much more sane than ruby.</p>

<p>tests can be run with: <code>py.test -v testinfra/ssh.py</code></p>

<p>Tests can be run from the salt master and use the salt API for running tests on minions. In this way we are not reliant on services like SSH which we cant guarantee will be working. If the salt connection is also down, either the server is off, removed or is otherwise unresponsive and requires admin intervention. Although it is more likely that you would be alerted to this by the monitoring infrastructure.</p>

<p>Alternatively the ssh transport can be used which uses the ssh command available on $PATH, which should be used by default.</p>


			<aside class="copyright" role="note">
				
				Documentation built with
				<a href="https://www.gohugo.io" target="_blank">Hugo</a>
				using the
				<a href="http://github.com/digitalcraftsman/hugo-material-docs" target="_blank">Material</a> theme.
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
  
      <a href="/grim/homelab/" title="Handling Go Dependencies">
        <span class="direction">
          Previous
        </span>
        <div class="page">
          <div class="button button-previous" role="button" aria-label="Previous">
            <i class="icon icon-back"></i>
          </div>
          <div class="stretch">
            <div class="title">
              Handling Go Dependencies
            </div>
          </div>
        </div>
      </a>
  
  </div>

  <div class="next">
  
      <a href="/grim/overview/" title="Overview">
        <span class="direction">
          Next
        </span>
        <div class="page">
          <div class="stretch">
            <div class="title">
              Overview
            </div>
          </div>
          <div class="button button-next" role="button" aria-label="Next">
            <i class="icon icon-forward"></i>
          </div>
        </div>
      </a>
  
  </div>
</nav>





			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = '';
      var repo_id  = '';
    
    </script>

    <script src="/grim/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    <script src="/grim/javascripts/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

